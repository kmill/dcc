\documentclass[11pt]{article}

\usepackage{amsmath, amsfonts,amssymb,amsthm}
\usepackage{fullpage}

\title{6.035 Project 4 Writeup}
\author{Kyle Miller, Alec Thomson, Patrick Hulin, Steven Valdez}
\begin{document}
\maketitle

\section {Instructions for Building and Running the Project}

\section {Overview}



\section {Division of Work}

The work was divided as follows. Kyle Miller designed and implemented
the register allocator along with a slew of peephole optimizations
that occurred at the code generation level. Kyle also wrote part of
the code generator portion of the loop parallelizer and wrote web
analysis for the Mid IR that was essential for loop analysis. Alec Thomson wrote most of
the loop analysis routines, including all of the loop parallelization
analysis, the dominator analysis, the loop nesting analysis, and
several loop heuristics that were used by the register allocator and
code generator. Alec
also wrote the expression unflattener to aid with instruction
selection. Patrick Hulin helped write a portion of the loop
parallelizing code generator, wrote some loop invariant code analysis
that ultimately didn't make it in to the final product, and helped
manage the considerable test-suite. Steven Valdez wrote an expression
optimization involving elimination of conditional statements and also
helped manage the test-suite. 

\section{Assumptions}

The following assumptions were made about the Decaf specification.
All of them are arbitrary, and so we feel no need to justify them.
\begin{itemize}
\item The array index expression is evaluated before the assignment
  expression.
\item Binary operators are evaluated left to right.
\item Loop bounds are evaluated from left to right.
\item Method arguments are evaluated right to left.  This was chosen
  when we still used \texttt{push} for stack-based arguments.
\end{itemize}

We assumed a stack-based calling convention in that the stack may
overflow during a call.  However, we did not require stack-based
semantics for method invocation.  For instance, with tail-call
elimination, some programs which would have overflowed the stack were
transformed into programs which no longer did so.  The reason we
assumed stack-based calling semantics was for its straightforwardness
as well as to be able to use C calling conventions for
\texttt{callout}s.

We assumed that division by zero is a run time error in addition to
the array bounds checking and control-flow falloff checking.
Furthermore, we required that a program which failed due to a division
by zero never be transformed into a program which did not fail in such
a manner.  This assumption is required to reason about transformations
like dead code elimination which may remove divisions.  We made this
assumption for consistency: whether a program fails should not be
contingent on whether we performed dead code elimination.  This may
seem contradictory to letting tail call optimization to remove stack
overflows, but stack-based semantics are not part of Decaf (one could
imagine a heap-based calling convention, for instance), but division
semantics are implicitly imported into Decaf by virtue of having
division.

The specification is contradictory as to what may be passed as an
argument to a callout.  At some point it mentions that arrays may be
passed to callouts.  The semantics we chose were that arrays without
square brackets evaluate to the pointer to the first element of the
array.  Furthermore, we assumed that an array is a contiguous block of
$8n$ bytes, where $n$ is the size of the array.  We chose this
convention because it was easy to generate code to interface with such
a convention, and because then the arrays could be passed to callouts
assuming C conventions on the arrays.

The specification was unclear on scoping rules.  We decided on the
following scoping rules.  Scopes are nested, with the global scope as
its root.  There is a method scope for each method with the global
scope as its parent scope.  Each method scope contains as bindings the
formal parameters to the method.  Each block and \texttt{for} loop
defines a new scope, lexically nested.  There may not be multiple
bindings with the same name in any given scoping level.  These rules
admit the following program:
\begin{verbatim}
class Program {
  int x;
  void f(int x) {
    int x;
  }
  void main() { }
}
\end{verbatim}
The specification states that ``the method scope consists of names of
variables and formal parameters introduced in the method declaration''
as well as that ``additional scopes exist within each
$\langle$block$\rangle$ of code''.  But, since a method definition has
no variables, and because the grammar says that the declaration is
followed by a $\langle$block$\rangle$, this means that a method scope
has no variable names in addition to its formal parameters.
Therefore, we accept the above program as valid.  This assumption is
contrary to the assumptions in one of the semantic test cases.

\section{Intermediate Representations}

Our compiler mainly uses two intermediate representations which we
call the mid-IR and the low-IR.  The mid-IR looks roughly like what we
used regularly in class, which consists of expressions, assignments,
and branches. The low-IR is essentially assembly in a control flow
graph.

The various IRs are all instances of control flow graphs parameterized
by the types of instructions found in the graph.  The graphs
themselves are composed of \texttt{Block}s, which are tree-like data
structures consisting of the instructions at their leaves, and which
have the constraint that a label instruction occurs at the beginning
of the block, a jump-like instruction occurs at the end, and neither
such kinds of instructions occur in the middle.

The mid-IR has side-effect-free expression trees contained in possibly
side-effect-full instructions, where the instructions are the leaves
in the \texttt{Block} structures.  We chose this distinction to ensure
that optimizations such as dead code elimination could remove
instructions which were determined to be side-effect-free without
having to inspect the contained subexpressions.

As a consequence to this distinction, division was implemented as an
instruction instead of as an expression because there was the
possibility for division to kill the program with a divide-by-zero
error.  We contemplated adding a division expression for those
divisions which were proved to never divide by zero, but we never got
around to implementing an analysis which would make this useful.

Because the program has been semantically checked before it is
converted to the mid-IR representation, we could simplify the type
system and assume that 64-bit integers were the only type, using C
conventions for booleans.  This simplification reduced the complexity
of the intermediate representations because we had to deal with fewer
kinds of instructions and because we did not have to keep track of
type information.

In the low-IR, there was a data constructor for each kind of assembly
instruction.  We had the type system enforce the constraints on the
kinds of operands which were accepted by the instruction.  For
instance, there were three constructors for the \texttt{mov}
instruction: \texttt{MovIRMtoR}, \texttt{MovIRtoM}, and
\texttt{Mov64toR}.  These correspond to various combinations of moving
immediates, registers, and memory.  The last is a special 64-bit
instruction which moves a 64-bit rather than 32-bit literal into a
register.  This type enforcement prevented us from making mistakes
when constructing assembly in the code generator.

\section {Algebraic Simplification} 


\section {Expression Unflattener}

The purpose of the expression unflattener is to return the mid-ir to
``Tree-Form'' after CSE is performed while maintaining the benefits
gained from CSE so that optimized Instruction Selection can be easily
performed by the code generator. The design, implementation, and test
plan of the unflattener are described below.

\subsection{Design of Expression Unflattener}

Our design first makes use of the liveness information
provided by dead-code elimination to determine which variables are
live at any given block in the mid-ir control flow graph. Once this
information is obtained, unflattening can be performed at the block
level.\\


\noindent Each block is then re-written according to the following algorithm: 

\begin{verbatim}
UnflattenBlock(block):

1. Determine the number of uses of each variable in this block 
2. Determine the reaching definitions of each variable defined in this
block
3. For each variable of each expression in this block: 
   1. If the variable is not live at any of this blocks successors 
      AND the variable is only used once in this entire block 
      (i.e. this is the only use)
      AND the variable is mapped to a reaching definition from this block 
   4. THEN replace the use of this variable with the expression from
its reaching definition
\end{verbatim}

\noindent The UnflattenBlock function is run on the same block until that block
reaches a fixed point. The general purpose of this algorithm is to
discover variables that are ``temporary'' in a given block and to replace
their uses with the expressions they were assigned to represent during
the flattening phase. Since the algorithm doesn't consider a variable
``temporary'' if it is used more than once (or if it is live in a
successor of this block), the benefits of CSE should be preserved
while the mid-ir is returned to tree-form. 

\subsection{Implementation of Expression Unflattener}

The expression unflattener is implemented in the Dataflow.hs source
file. The algorithm is roughly the same as the one described above
with some augmentations to allow for easy fixed point detection.

The first thing the implementation does is use the Hoopl liveness
analysis implemented as part of the previous project to obtain a
factbase of the live variables at the start of every block. Next it
maps the ``UnflattenBlock'' function across every block in the graph
to obtain a new graph with every block flattened to a fixed point. The
``UnflattenBlock'' function will call itself recursively if it detects
that it is returning a changed graph as a result of its
modifications. When ``UnflattenBlock'' no longer has an effect, the
algorithm terminates.

After unflattening every block in the graph, the algorithm then
concludes by performing dead code elimination again to eliminate
assignments to temporaries that are no longer being used.

\subsection{Experimental Results of Expression Unflattener}

The unflattener passes our entire test suite and appears to work
correctly. The figures below show the unflattener operating on a
simple program. The second figure shows how the unflattener maintains
the benefits obtained from CSE. 

% TODO: Put the before and after graphs right here for unflattentest1 and unflattentest2

\section {Conditional Elimination} 


\section {Register Allocation}

We mainly followed the paper ``Iterated Register Coalescing'' by Lal
George and Andrew Appel, which itself based on the Briggs and Chaitlin
algorithms.  We performed the register allocation at our low
intermediate representation, which was essentially assembly.

The first thing which needed to be done was to make it so we could
refer to particular program points.  Our intermediate representations
are built 

\section{Loop Parallelizer}

The loop parallelizer splits the control flow into multiple threads
for loops with expensive bodies with no inter-run dependencies. The
analysis runs at the mid IR level 

The design of the loop parallelizer is split into two major parts,
loop analysis and code generation. The loop analysis portion is
concerned with discovering loops that can be parallelized while making
no transformations to the IR. The loop analysis portion is also
responsible for running a heuristic that determines whether loops
should be parallelized considering the overhead of thread
creation. The code generation portion uses the analysis information to 
generate appropriate code to split the loops among many different
threads.

\subsection { Loop Analysis } 

Loop analysis is a multi-stage process that operates on the mid IR
after most of the Mid IR dataflow transformations have completed. We
chose to operate on the Mid IR because it allowed us to parallelize
for-loops and while-loops equally while reaping the full benefits of
constant and copy propagation. The major stages of loop analysis,
described in detail below, are Dominator Analysis, Loop
Identification, Induction Variable Identification, Loop Invariance
Analysis, and Cross Dependency Analysis. Ultimately, the analyzer uses
the integer programming method with the help of the glpk-hs library to
solve the cross dependency constraints. 

\subsubsection {Dominator Analysis} 

A useful first step in performing loop analysis was to discover the
dominator map of the midir. To do so, we used a simple forwards
dataflow analysis using Hoopl. The fact type was a set of dominators
while the join function was set intersection with bottom being
represented by the abstract ``Bot'' type. 

The transfer function was very simple and only updated the set of
dominators at entry and exit points of the blocks. Ultimately, Hoopl
took care of most of the analysis and there was no need to rewrite any
portion of the graph. 

The dominator map produced by this analysis was then a map from
block labels to the sets of dominators of those blocks. The dominator
map proved essential to the discovery of loops, loop induction
variables, and loop invariant expressions and variables. 

\subsubsection {Loop Identification}

Once dominator analysis was complete, loops could be identified by
what Muchnick refers to as a ``Back-Edge''. A back-edge is any edge in
the control flow graph that points from a block (referred to as the
``loop-back'' in our code) to one of its dominators (referred to as
the ``loop-header''). Once a back-edge has been identified, the body
of the ``Natural Loop'' can be found by looking for all of the blocks
that are capable of reaching the loop-back \emph{without} travelling
through the loop-header.  

To find all of the blocks in the loop body, another custom dataflow
analysis was performed called ``loop-reaching'' analysis that
ultimately identified all the blocks in the control-flow graph that
could reach the loop-back without travelling through the
loop-header. This was a backwards dataflow analysis with a boolean
fact type that returned the set of blocks belonging to the natural
loop body. 

After the body of the natural loop was discovered, additional blocks
needed to be added to the loop body that were not caught by the
reaching analysis. These blocks were those that ended in return or
fail statements and thus wouldn't be identified as part of the natural
loop but should be considered to be part of the loop body. In
particular fail statements were an essential part of the loop body as
they were placed there by run-time array bounds checks. 

\subsubsection {Induction Variable Identification} 

After loop bodies were identified, the next was identification of the
base induction variables. Since loop analysis is done at the Mid-IR,
identification of base induction variables is slightly more
complicated than just looking at the variable used in the decleration
of a for loop. Fortunately, since the process identifies any base
induction variable without looking at high level code, while loops and
for loops can be analyzed identically. For example, at the mid ir
level, the code 

\begin{verbatim} 

for(i=0; N) { 
// Do Stuff
}

\end{verbatim}

\noindent is effectively identical to 

\begin{verbatim} 

i=0; 
while(i < N) { 
// Do Stuff 
i+=1;
}

\end{verbatim} 

\noindent To aid in identification of induction variables, a set of
mid level ``webs'' were constructed for the mid ir graph. The mid
level webs are a set of Def-Use chains that are essentially the webs
defined in the register allocation lecture. Once a set of webs has
been produced for the entire mid level graph, 

\subsubsection {Loop Invariance Analysis} 


\subsubsection {Dependency Analysis}

\subsection{Code Generation}

% stuff about loop analysis

The parallelizer makes use of a new instruction in our mid-ir,
"Parallel". This instruction is inserted before the loop, and the
loop's induction variable update instructions are modified in order to
only execute some of the loop's executions in each run.

% stuff about code generation

\subsection{Results}

Below are some examples of some code that gets generated by the
parallelizer. With a loop cost threshold of about 1000000, we found
that parallelization could provide up to a 75\% speedup on
specifically designed tests (such as matrix multiply). On test cases
that were almost exclusively parallel loops, the parallelizer could
produce speedup results close to the number of cores on the target
machine. 

% add some graphs

\section{Loop Invariant Code Motion}

Loop Invariant Code Motion (LICM) moves loop-invariant instructions
out of loops so they execute less frequently. The optimization is
important because some compiler-generated instructions are
loop-invariant and get repeated over and over, which is clearly a
waste of resources. The implementation consists of a dataflow analysis
and a non-local rewrite function: the rewrite function uses the State
monad to ensure that instructions only get moved once.

\section{Conclusion}


\end{document}
