\documentclass[11pt]{article}

\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{fullpage}

\title{6.035 Project 4 Design Document}
\author{Kyle Miller, Alec Thomson, Patrick Hulin, Steven Valdez}
\begin{document}
\maketitle

\section {Overview} 

For the final optimization project of our compiler, we plan to split
the work into several significant chunks of optimizations to make good
use of the large size of our team. The two major optimizations we plan
to implement include a Register Allocator (Section~\ref{sec:register}) and a Loop
Parallelizer (Section~\ref{sec:parallel}). Smaller optimizations we plan to implement include
Loop-Invariant Code Motion (Section~\ref{sec:codehoist}), Algebraic
Simplification (Section~\ref{sec:algebra}), Tailcall Optimizations
(Section~\ref{sec:tailcall}), an Expression
Unflattener (Section~\ref{sec:unflatten}), and a variety of Peephole
Optimizations (Section~\ref{sec:peephole}). 

Our test plan (Section~\ref{sec:test}) is designed to test for correctness on many levels and
to provide us with useful information about the effectiveness of our
optimizations. Finally, the order in which we perform these
optimizations is discussed in Section~\ref{sec:order}.

\section {Division of Work}

Right now, our plan is to split the optimizations amongst ourselves as
follows: Kyle Miller will design and implement the Register
Allocator, Tailcall Optimizer, and Algebraic Simplifications. Alec Thomson will design and implement the Loop
Parallelizer and the Expression Unflattener. Steven Valdez will design and implement the Peephole
Optimizations. Finally, Patrick Hulin will design and implement
Loop-Invariant Code Motion and be in charge of the test system. 

\section {Register Allocator}
\label{sec:register}

Register allocation is the most important optimization we hope to
implement for our optimized compiler. Fortunately, it is a very
well-defined and well-documented problem, so we plan to make use of
extensive materials while implementing a standard solution. 

The current plan for our register allocator, which operates at nearly
the lowest level of our compiler, is to implement the
algorithm as described by Muchnick in the register allocation
section. The final register allocator will be a combination of
liveness analysis and graph coloring with a set of heuristics to
approximate the optimal solution to the graph coloring problem. 

We expect to see the code generator output 20-25\% fewer instructions
then it currently does, the largest speedup by far. Most instructions
currently are superfluous moves to and from the stack.

\section {Loop Parallelizer}
\label{sec:parallel}

The purpose of the Loop Parallelizer is to make better use of a multicore
system by splitting up independent loops into multiple threads of
execution. The general idea is that if each iteration of a loop does
not depend on any of the other iterations, the order of execution of
the iterations does not matter and each iteration can be run on a
different core of the system. For example, the following loop could be
parallelized 

\begin{verbatim}
for (i = 0 : N) {
  A[i] = i; 
}
\end{verbatim} 

Our loop parallelizer will operate on the mid-ir by first determining
which loops can be parallelized and then annotating appropriate blocks
with instructions to parallelize. The code generator will then
systematically create code to generate extra threads of execution upon encountering
these annotations.

To find the loops in our mid-ir, we can make use of our dataflow
framework from the previous project to do ``Dominator Analysis''. 

Since Loop parallelization can be quite tricky, our initial design
only focuses on a select subset of parallelizable loops. For example,
our initial design only parallelizes
loops whose iterations are \textbf{completely independent} of each
other in terms of data used. Such a design hopes to minimize the
number of locks used and avoid complexity such as message passing in
between dependent iterations.\\

Finally, since the decaf language doesn't allow the arbitrary use of
pointers, variable alias analysis (where variables are analyzed to
determine if they represent the same memory location) will not be
necessary, further simplifying the analysis of loop parallelization. 

In terms of implementation, we plan to make use of the provided C
libraries to perform the actual generation of threads and locking of
shared memory structures.  

\section {Expression Unflattener}
\label{sec:unflatten}

The Expression Unflattener is an optimization that returns the
Control-Flow Graph to ``Tree-form'' after CSE is performed. As
explained in our previous project writeup, the mid-ir is initially constructed
in tree-form (where expressions are not flattened into simple unary
and binary op forms) because the code generator can pattern match over
expression trees to determine the best possible assembly
instruction to use for certain expressions. Tree-form is particularly
useful for memory addressing as the code generator can make use of
complex x86 move instructions when possible.\\

To perform CSE during the last project, the mid-ir had to first be
``Flattened'' so that common subexpressions could be identified. For
example, an expression such as 

\begin{verbatim}
x := i + (j * y)
\end{verbatim} 

\noindent would be flattened to 

\begin{verbatim}
t := j * y 
x := i + t
\end{verbatim}

To make effective use of our code generator and to reap the full
benefits of CSE, this flattened form of the mid-ir should be restored
to tree-form while maintaining all of the benefits achieved by CSE. 

To do so, our design first makes use of the liveness information
provided by dead-code elimination to determine which variables are
live at any given block in the mid-ir control flow graph. Once this
information is obtained, unflattening can be performed at the block
level.\\


\noindent Each block is then re-written according to the following algorithm: 

\begin{verbatim}
UnflattenBlock(block):

1. Determine the number of uses of each variable in this block 
2. Determine the reaching definitions of each variable defined in this
block
3. For each variable of each expression in this block: 
   1. If the variable is not live at any of this blocks successors 
      AND the variable is only used once in this entire block 
      (i.e. this is the only use)
      AND the variable is mapped to a reaching definition from this block 
   4. THEN replace the use of this variable with the expression from
its reaching definition
\end{verbatim}

\noindent The UnflattenBlock function is run on the same block until that block
reaches a fixed point. The general purpose of this algorithm is to
discover variables that are ``temporary'' in a given block and to replace
their uses with the expressions they were assigned to represent during
the flattening phase. Since the algorithm doesn't consider a variable
``temporary'' if it is used more than once (or if it is live in a
successor of this block), the benefits of CSE should be preserved
while the mid-ir is returned to tree-form. 

\section {Tail-Call Recursion}
\label{sec:tailcall}

The Tail-Call Recursion optimization is an optimization that avoids
allocating a new frame in the stack because the caller will simply
return the value of the call. In the ``tree-form'' of the MidIR, this
optimization would eliminate calls of this form, and instead create an
edge between the caller and inside the callee.\\

Since Tail-Call Recursion can have side effects, and is inherently
difficult to construct a lattice for across multiple methods, our
initial design focuses on only Tail-Call Recursion across calls with
the \textbf{same caller and callee methods}. In future revisions of
this optimization, we hope to increase the optimization across
seperate methods in order to take more advantage of this optimization
in non-self recursive methods.

To perform this optimization, for each method in the program, we keep
track of the returns from each method, and create a Lattice with the
Return type of each method, and then apply the Tail-Call optimization
to calls at the end of methods that match the form where the caller
and callee are both the same method, and the current pass over the
Lattice is focusing on that function. Once we've found a location
where a Tail-Call optimization can be performed, we reconstruct the
midir graph to remove the call from that block, and add an edge
between the node with the caller, and a node with the callee.


\section {Algebraic Simplification}
\label{sec:algebra}

Algebraic Simplification is the process of simplifying basic algebraic
identities (such as $x := y * 1$) to
produce simpler code. 

Our algebraic simplifier is a basic extension of the constant
propagator we produced as part of the previous project. We make use of
Hoopl's graph rewrite functionality to check a given expression
against a series of simplification rules we define in a separate
module. Our rules are defined by a DSL and we require a valid
justification for each rule since some rules can have unintended
consequences (such as attempting to negate the most negative
integer). 

To avoid producing incorrect code as a result of algebraic
simplification, we plan to change the mid-ir so that ``Division
Operations'' (which include both divides and mods) are instructions
separate from ``pure'' expressions. The reason for this is that
division operations are capable of producing side-effects in the form
of divide by zero exceptions, so our algebraic simplifier is safer
when it only operates on ``pure'' expressions. 



\section {Loop Invariant Code Motion}
\label{sec:codehoist}

When a single variable is assigned the same value repeatedly in a
loop, it is a good idea to move that assignment outside the loop.
Specifically, our Loop Invariant Code Motion optimization moves
statements which are invariant over loop evaluations to the previous
basic block. This invariant determination can be easily made using
reaching definitions analysis: if the reaching definitions for all the
variables on the right-hand side of an assignment are before the loop,
then we can move the assignment. 

\section {Peephole Optimizations and other Small Optimizations}
\label{sec:peephole}
In addition to the above major optimizations, we additionally plan to
implement various Peephole Optimizations and other smaller
optimizations to further improve the performance of the assembled
code.\\

The Peephole optimizations that we intend to perform include Strength
Reduction, Constant Folding, Null Sequence optimizations. Strength
Reduction is the optimization where slower optimizations are replaced
by faster optimizations, such as replacing:\\

\begin{verbatim}
a = b * 4;
t = 0;
for(i=0;10)
  t += i * a;
\end{verbatim}

with:\\

\begin{verbatim}
a = b >> 2;
t = 0;
k = 0;
for(i=0;10)
{
  t += k;
  k += a;
}
\end{verbatim}

Meanwhile, Constant Folding is the optimization where $10*2*30$ is
immediately evaluated as $600$. This would reduce the amount of
arithmetic operations that need to be done at run-time and rather move
as many of them as possible to compile-time.

The Null Sequence optimization, which we already partially do, is an
optimization that would remove any code that has no effect on the
running code, and that has no side-effects throughout the
program. This is a lower level version of Dead Code Elimination, which
we are planning to run at the Assembly level in order to remove any
remaining dead code that is produced from the conversion of the midir
graph to the Assembly Code.

Another ``optimization'' that we're doing is Instruction Selection,
where we select instructions at the Assembly level that are faster
than other generalized forms of these instructions. While similar to
Strength Reduction, this level of ``optimization'' is one that is
specific to the assembly level implementation of the instructions,
whether using ``xor \%rax, \%rax'' rather than ``mov 0, \%rax''. Or,
using various forms of ``lea'' rather than ``mov'', ``cmov''.  These
optimizations, while not very effective overall, still form a critical
part of the code generation, since it allows us to eliminate assembly
instructions that would end up stalling the pipeline at the CPU level.

\section {Test Plan}
\label{sec:test}

Our test plan consists of several redundant factors to increase the
probability that we catch any errors with our compiler. First, our
test suite includes a set of shell scripts that make use of every test
we've written inside a certain ``testing directory''. We also plan to
implement a ``Mid-IR to C'' compiler that produces valid C code from a
valid decaf program's middle IR and uses GCC to produce the expected
output. Finally, to test correctness of our compiler even further, we
also plan to implement a complex and ``Real'' decaf program, which in
our case is a VM for a very simple stack-based programming language.

\subsection {Testing Scripts}

All the tests from each stage have been compiled into a single
directory with a single testing script. This script will run the test,
and if that test fails, it will run the Mid-IR to C converter with no
optimizations to determine whether the problem is a result of
optimizations or a result of code generation. 

\subsection {Mid-IR to C Converter} 

The Mid-IR to C converter converts from our middle IR to C. The Mid-IR
preserves the function call abstraction, but not internal control
flow. The C code is human readable, and enables us to see the results
of programs before we do register allocation and code generation.

\subsection {Large ``Real'' Decaf Program}


\section {Ordering of Optimizations} 
\label{sec:order}

The order of the optimizations is important for their
effectiveness. The general order of our optimizations, which we derived
in part from Muchnick, is as follows: 

\begin{enumerate}

\item Constant Propagation immediately followed by Algebraic
  Simplification 

\item Initial Tail Call Optimizations 

\item Elimination of superfluous blocks 

\item Global CSE (with flattening of the mid-ir)

\item Global Copy Propagation

\item Dead Code Elimination

\item Unflattening of the mid-ir 

\item Dead Code Elimination again to remove anything missed by the
  unflattener 

\item Loop Invariant Code Motion

\item Another Tail Call pass 

\item Another Dead Code Elimination pass 

\item Another Block elimination pass 

\item Loop Parallelization

\item Initial Peephole Optimizations including peephole instruction selection

\item Register Allocation

\item Peephole Optimizations again including more instruction selection

\end{enumerate}

\noindent It is our goal with this ordering to produce the best possible use of
our individual optimizations. 

\end{document}

